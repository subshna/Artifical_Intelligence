Creating a Virtual Machine
***************************
gcloud auth list
gcloud config list project

sudo su -
apt-get update

apt-get install nginx -y
ps auwx | grep nginx

gcloud compute instances create <instance name> --zone <zone name>
gcloud compute ssh <instance name> --zone <zone name>

****************

gcloud auth list
gcloud config list project
gcloud -h
gcloud config --help
gcloud help config
cd $HOME
vi ./.bashrc
gcloud config list
gcloud config list --all

# Try creating a Cloud Storage bucket. Bucket names must be unique, so replace unique-name with something else, or append the name to make it unique.
gsutil mb gs://<unique-name>

First, create a test file:
vi test.dat
Add some data to your file:

# Now upload some data to the bucket you created (make sure to replace "unique-name" with your storage bucket):
gsutil cp test.dat gs://unique-name

***************
#Creating a Persistent Disk

gcloud auth list
gcloud config list project
gcloud compute instances create gcelab --zone us-central1-c

#Create a new persistent disk
gcloud compute disks create <mydisk> --size=200GB \
--zone us-central1-c

#attach the new disk (mydisk) to the virtual machine instance you just created (gcelab).
gcloud compute instances attach-disk gcelab --disk <mydisk> --zone <us-central1-c>

#Finding the persistent disk in the virtual machine
#SSH into the virtual machine:
gcloud compute ssh gcelab --zone us-central1-c
#press enter twice for entering the passpharse

#Now find the disk device by listing the disk
ls -l /dev/disk/by-id/

#Formatting and mounting the persistent disk
#disk, format it, and then mount it using the following Linux utilities:
#mkfs: creates a filesystem
#mount: attaches to a filesystem
sudo mkdir /mnt/mydisk
sudo mkfs.ext4 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/disk/by-id/scsi-0Google_PersistentDisk_persistent-disk-1

#Now use the mount tool to mount the disk to the instance with the discard option enabled:
sudo mount -o discard,defaults /dev/disk/by-id/scsi-0Google_PersistentDisk_persistent-disk-1 /mnt/mydisk

#By default the disk will not be remounted if your virtual machine restarts. To make sure the disk is remounted on restart, you need to add an entry into /etc/fstab
sudo nano /etc/fstab
# Add the following below the line that starts with "UUID=..."
/dev/disk/by-id/scsi-0Google_PersistentDisk_persistent-disk-1 /mnt/mydisk ext4 defaults 1 1

*******************************
#Hello Node Kubernetes
gcloud auth list
gcloud config list project

#Create your Node.js application
vi server.js

#Add this content to the file:
var http = require('http');
var handleRequest = function(request, response) {
  response.writeHead(200);
  response.end("Hello World!");
}
var www = http.createServer(handleRequest);
www.listen(8080);

#Since Cloud Shell has the node executable installed, run this command (the command produces no output):
node server.js

#Create a Docker container image
#Next, create a Dockerfile that describes the image you want to build. Docker container images can extend from other existing images, so for this image, we'll extend from an existing Node image.
vi Dockerfile

#Add this content
FROM node:6.9.2
EXPOSE 8080
COPY server.js .
CMD node server.js

#Build the image with the following, replacing PROJECT_ID with your lab project ID, found in the Console:
docker build -t gcr.io/<PROJECT_ID>/hello-node:v1 .

#Once complete, test the image locally with the following command which will run a Docker container as a daemon on port 8080 from your newly-created container image (replacing PROJECT_ID with your lab project ID, found in the Console):
docker run -d -p 8080:8080 gcr.io/<PROJECT_ID>/hello-node:v1

#Or use curl from your Cloud Shell prompt:
curl http://localhost:8080

#Find your Docker container ID by running:
docker ps

#Stop the container by running the following, replacing the CONTAINER ID provided from the previous step:
docker stop <CONTAINER ID>

#Now that the image is working as intended, push it to the Google Container Registry, a private repository for your Docker images, accessible from your Google Cloud projects.

#Run this command, replacing PROJECT_ID with your GCP Project ID, found in the Console
gcloud docker -- push gcr.io/<PROJECT_ID>/hello-node:v1

#Create your cluster
gcloud config set project <PROJECT_ID>

#Create a cluster with two n1-standard-1 nodes (this will take a few minutes to complete):
gcloud container clusters create hello-world \
                --num-nodes 2 \
                --machine-type n1-standard-1 \
                --zone us-central1-a
#If you click on Navigation menu > Kubernetes Engine, you'll see that you now you have a fully-functioning Kubernetes cluster powered by Kubernetes Engine:

#Create your pod
#A Kubernetes pod is a group of containers tied together for administration and networking purposes. It can contain single or multiple containers. Here you'll use one container built with your Node.js image stored in your private container registry. It will serve content on port 8080.

#Create a pod with the kubectl run command (replace PROJECT_ID with your lab Project ID, found in the console):
kubectl run hello-node \
    --image=gcr.io/<PROJECT_ID>/hello-node:v1 \
    --port=8080
#To view the deployment, run:
kubectl get deployments

#To view the pod created by the deployment, run:
kubectl get pods

#Now is a good time to go through some interesting kubectl commands. None of these will change the state of the cluster, full documentation is available here:
kubectl cluster-info
kubectl config view

#And for troubleshooting :
kubectl get events
kubectl logs <pod-name>

#Allow external traffic
#By default, the pod is only accessible by its internal IP within the cluster. In order to make the hello-node container accessible from outside the Kubernetes virtual network, you have to expose the pod as a Kubernetes service.

#From Cloud Shell you can expose the pod to the public internet with the kubectl expose command combined with the --type="LoadBalancer" flag. This flag is required for the creation of an externally accessible IP:
kubectl expose deployment hello-node --type="LoadBalancer"

#To find the publicly-accessible IP address of the service, request kubectl to list all the cluster services:
kubectl get services

#Scale up your service
#One of the powerful features offered by Kubernetes is how easy it is to scale your application. Suppose you suddenly need more capacity for your application. You can tell the replication controller to manage a new number of replicas for your pod:
kubectl scale deployment hello-node --replicas=4

#You can request a description of the updated deployment:
kubectl get deployment

#kubectl get deployment
kubectl get pods

#Roll out an upgrade to your service
#At some point the application that you've deployed to production will require bug fixes or additional features. Kubernetes helps you deploy a new version to production without impacting your users.

#First, modify the application. Edit the
vi server.js
Edit the file "hello kubernetes world:

#Run the following, replacing PROJECT_ID with your lab project ID, found in the Console:
docker build -t gcr.io/PROJECT_ID/hello-node:v2 .
gcloud docker -- push gcr.io/PROJECT_ID/hello-node:v2
#Kubernetes will smoothly update your replication controller to the new version of the #application. In order to change the image label for your running container, you will #edit the existing hello-node deployment and change the image from #gcr.io/PROJECT_ID/hello-node:v1 to gcr.io/PROJECT_ID/hello-node:v2.

#Run the following to update the deployment with the new image. New pods will be created with the new image and the old pods will be deleted.
kubectl get deployments

node cloudiot_mqtt_example_nodejs.js \
    --projectId=qwiklabs-gcp-92de07ed174d2a88 \
    --registryId=my-registry \
    --deviceId=my-device \
    --privateKeyFile=rsa_private.pem \
    --numMessages=25 \
    --algorithm=RS256 \
    --mqttBridgePort=443
gcloud pubsub subscriptions create \
projects/qwiklabs-gcp-92de07ed174d2a88/subscriptions/my-subscription \
    --topic=projects/qwiklabs-gcp-92de07ed174d2a88/topics/my-device-events